## 设置您的缓存类型

默认情况下，Auto-GPT将使用LocalCache而不是redis或Pinecone。

To switch to either, change the MEMORY_BACKEND env variable to the value that you want:

- local（默认）使用本地JSON缓存文件
- pinecone使用您在ENV设置中配置的Pinecone.io帐户
- redis将使用您配置的redis缓存

## 内存后端设置

### Redis设置

> 谨慎 
>  这不打算公开访问，并且缺乏安全措施。因此，避免在没有密码或根本没有密码的情况下将Redis暴露在互联网上

1. 安装docker（或Windows上的Docker Desktop）。
2. 启动Redis容器。

```
 docker run -d --name redis-stack-server -p 6379:6379 redis/redis-stack-server:latest
```

有关设置密码和其他配置，请参阅https://hub.docker.com/r/redis/redis-stack-server。

1. 在.env设置以下设置。 在角度括号中替换密码（<>）

```
MEMORY_BACKEND=redis

REDIS_HOST=localhost

REDIS_PORT=6379

REDIS_PASSWORD=<PASSWORD>

You can optionally set `WIPE_REDIS_ON_START=False` to persist memory stored in Redis.
```

您可以使用以下方式指定redis的内存索引：

```
MEMORY_INDEX=<WHATEVER>
```



### 🌲 Pinecone API密钥设置

Pinecone允许您存储大量基于矢量的内存，允许代理在任何给定时间仅加载相关内存。

1. 如果你还没有，就去[Pinecone](https://app.pinecone.io/)开一个账户。
2. 选择Starter计划以避免被收费。
3. 在左侧边栏的默认项目下找到您的API密钥和区域。

在.env文件集中：

- PINECONE_API_KEY
- PINECONE_ENV（例如*：“us-east4-gcp”）*
- MEMORY_BACKEND=pinecone

或者，您可以从命令行（高级）设置它们：

对于Windows用户：

```
setx PINECONE_API_KEY "<YOUR_PINECONE_API_KEY>"

setx PINECONE_ENV "<YOUR_PINECONE_REGION>" # e.g: "us-east4-gcp"

setx MEMORY_BACKEND "pinecone"
```

对于macOS和Linux用户：

```
export PINECONE_API_KEY="<YOUR_PINECONE_API_KEY>"

export PINECONE_ENV="<YOUR_PINECONE_REGION>" # e.g: "us-east4-gcp"

export MEMORY_BACKEND="pinecone"
```





## 查看内存使用情况

> 使用--debug标志查看内存使用情况:)

### 🧠内存预播

> 内存预播允许您将文件摄取到内存中，并在运行Auto-GPT之前预播。

```
# python data_ingestion.py -h 
usage: data_ingestion.py [-h] (--file FILE | --dir DIR) [--init] [--overlap OVERLAP] [--max_length MAX_LENGTH]

Ingest a file or a directory with multiple files into memory. Make sure to set your .env before running this script.

options:
  -h, --help               show this help message and exit
  --file FILE              The file to ingest.
  --dir DIR                The directory containing the files to ingest.
  --init                   Init the memory and wipe its content (default: False)
  --overlap OVERLAP        The overlap size between chunks when ingesting files (default: 200)
  --max_length MAX_LENGTH  The max_length of each chunk when ingesting files (default: 4000)

# python data_ingestion.py --dir DataFolder --init --overlap 100 --max_length 2000
```

在上面的示例中，脚本初始化内存，将Auto-Gpt/autogpt/auto_gpt_workspace/DataFolder目录中的所有文件摄取到内存中，100块和每块2000的最大长度重叠。

请注意，您还可以使用--file参数将单个文件摄取到内存中，data_ingestion.py只会摄取/auto_gpt_workspace目录中的文件。

DIR路径相对于auto_gpt_workspace目录，因此python data_ingestion.py --dir . --init将摄取auto_gpt_workspace目录中的所有内容。

您可以调整max_length和overlap参数，以微调当人工智能“调用”该内存时向其呈现文档的方式：

- 调整重叠值允许人工智能在调用信息时从每个块访问更多上下文信息，但会导致创建更多块，从而增加内存后端使用量和OpenAI API请求。
- 减少max_length值将创建更多的块，这可以通过在上下文中允许更多消息历史记录来保存提示令牌，但也会增加块的数量。
- 增加max_length值将为人工智能提供来自每个块的更多上下文信息，减少创建的块数量并保存在OpenAI API请求上。然而，这也可能使用更多的提示令牌，并减少人工智能可用的整体上下文。

内存预播是一种通过将相关数据摄取到内存中来提高人工智能准确性的技术。大数据被拆分并添加到内存中，允许人工智能快速访问它们并生成更准确的响应。它对于大型数据集或需要快速访问特定信息时非常有用。示例包括在运行Auto-GPT之前摄取API或GitHub文档。

⚠️如果您使用Redis作为内存，请确保在.env文件中使用WIPE_REDIS_ON_START=False运行Auto-GPT。

⚠️对于其他内存后端，我们目前在启动Auto-GPT时强制擦除内存。要使用这些内存后端摄取数据，您可以在Auto-GPT运行期间随时调用data_ingestion.py脚本。

记忆将在摄入时立即提供给人工智能，即使在Auto-GPT运行时摄入。